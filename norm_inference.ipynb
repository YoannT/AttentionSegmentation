{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"nlstruct\")\n",
    "logger.setLevel(logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'/home/ytaille/deep_multilingual_normalization')\n",
    "\n",
    "from nlstruct.utils import torch_clone\n",
    "from nlstruct.utils import torch_global as tg\n",
    "from deep_multilingual_normalization.preprocess import preprocess, load_quaero\n",
    "from deep_multilingual_normalization.train import train_step1, train_step2, clear\n",
    "from deep_multilingual_normalization.eval import compute_scores, predict\n",
    "\n",
    "from transformers import AutoModel\n",
    "from deep_multilingual_normalization.model import Classifier, FastClusteredIPSearch\n",
    "\n",
    "from notebook_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CUDA devices: 1\n",
      "Current device: cuda:0\n",
      "Using cache /home/ytaille/data/cache/preprocess_training_data/b5e7f1ec57d0ed68\n",
      "Loading /home/ytaille/data/cache/preprocess_training_data/b5e7f1ec57d0ed68/output.pkl... \n",
      "Loading MRCONSO...\n",
      "Deduplicating MRCONSO...\n",
      "French synonyms: 174317\n",
      "French labels: 84172\n",
      "Quaero mentions: 5714\n",
      "Mirrored labels: 84845\n",
      "Queried english labels: 84845\n",
      "Total deduplicated synonyms: 781723\n",
      "Total deduplicated labels: 84845\n",
      "Lock 139875591135304 acquired on /home/ytaille/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc.lock\n",
      "Lock 139875591135304 released on /home/ytaille/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc.lock\n",
      "Lock 139874431107928 acquired on /home/ytaille/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7.lock\n",
      "Lock 139874431107928 released on /home/ytaille/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7.lock\n",
      "Will train vocabulary for label\n",
      "Will train vocabulary for group\n",
      "Will train vocabulary for source\n",
      "Will train vocabulary for token\n",
      "Discovered existing vocabulary (105879 entities) for token\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Quaero mentions: 16283\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Normalized label, with given vocabulary and no unk\n",
      "Normalized quaero_source, with given vocabulary and no unk\n",
      "Normalized token, with given vocabulary and no unk\n",
      "Normalized group, with given vocabulary and no unk\n",
      "Normalized source, with given vocabulary and no unk\n",
      "Using cache /home/ytaille/data/cache/norm/paper/train_step1/f5a5142e2d206a3c\n",
      "Loading /home/ytaille/data/cache/norm/paper/train_step1/f5a5142e2d206a3c/history.yaml... \n",
      "epoch | train_loss | train_acc | val_loss | val_acc | val_map | acc_emea | acc_medline |        lr |   norm |     step |    dur(s)\n",
      "    1 |     \u001b[32m5.5433\u001b[0m |    \u001b[32m0.3453\u001b[0m |   \u001b[32m1.1686\u001b[0m |  \u001b[32m0.7679\u001b[0m |  \u001b[32m0.8366\u001b[0m |   \u001b[32m0.8390\u001b[0m |      \u001b[32m0.7039\u001b[0m | 8.000e-03 |   6.75 |     5274 |  603.8273\n",
      "    2 |     \u001b[32m1.9379\u001b[0m |    \u001b[32m0.7092\u001b[0m |   \u001b[32m0.6593\u001b[0m |  \u001b[32m0.8638\u001b[0m |  \u001b[32m0.9114\u001b[0m |   \u001b[32m0.9021\u001b[0m |      \u001b[32m0.8293\u001b[0m | 7.704e-03 |   9.14 |    10548 |  609.3077\n",
      "    3 |     \u001b[32m1.1637\u001b[0m |    \u001b[32m0.8329\u001b[0m |   \u001b[32m0.4806\u001b[0m |  \u001b[32m0.8925\u001b[0m |  \u001b[32m0.9347\u001b[0m |   \u001b[32m0.9188\u001b[0m |      \u001b[32m0.8689\u001b[0m | 7.111e-03 |  10.85 |    15822 |  606.0223\n",
      "    4 |     \u001b[32m0.8422\u001b[0m |    \u001b[32m0.8885\u001b[0m |   \u001b[32m0.3866\u001b[0m |  \u001b[32m0.9060\u001b[0m |  \u001b[32m0.9441\u001b[0m |   \u001b[32m0.9317\u001b[0m |      \u001b[32m0.8829\u001b[0m | 6.519e-03 |  12.06 |    21096 |  608.7320\n",
      "    5 |     \u001b[32m0.6649\u001b[0m |    \u001b[32m0.9166\u001b[0m |   \u001b[32m0.3409\u001b[0m |  \u001b[32m0.9116\u001b[0m |  \u001b[32m0.9498\u001b[0m |   \u001b[31m0.9273\u001b[0m |      \u001b[32m0.8975\u001b[0m | 5.926e-03 |  12.95 |    26370 |  604.6479\n",
      "    6 |     \u001b[32m0.5512\u001b[0m |    \u001b[32m0.9335\u001b[0m |   \u001b[32m0.3022\u001b[0m |  \u001b[32m0.9170\u001b[0m |  \u001b[32m0.9536\u001b[0m |   \u001b[32m0.9383\u001b[0m |      \u001b[32m0.8979\u001b[0m | 5.333e-03 |  13.61 |    31644 |  607.3276\n",
      "    7 |     \u001b[32m0.4713\u001b[0m |    \u001b[32m0.9442\u001b[0m |   \u001b[32m0.2749\u001b[0m |  \u001b[32m0.9186\u001b[0m |  \u001b[32m0.9554\u001b[0m |   \u001b[31m0.9332\u001b[0m |      \u001b[32m0.9055\u001b[0m | 4.741e-03 |  14.10 |    36918 |  609.0316\n",
      "    8 |     \u001b[32m0.4119\u001b[0m |    \u001b[32m0.9519\u001b[0m |   \u001b[32m0.2565\u001b[0m |  \u001b[32m0.9249\u001b[0m |  \u001b[32m0.9590\u001b[0m |   \u001b[31m0.9354\u001b[0m |      \u001b[32m0.9155\u001b[0m | 4.148e-03 |  14.46 |    42192 |  607.1812\n",
      "    9 |     \u001b[32m0.3662\u001b[0m |    \u001b[32m0.9574\u001b[0m |   \u001b[32m0.2438\u001b[0m |  \u001b[32m0.9253\u001b[0m |  \u001b[32m0.9596\u001b[0m |   \u001b[32m0.9428\u001b[0m |      \u001b[31m0.9095\u001b[0m | 3.556e-03 |  14.72 |    47466 |  609.2481\n",
      "   10 |     \u001b[32m0.3304\u001b[0m |    \u001b[32m0.9617\u001b[0m |   \u001b[32m0.2302\u001b[0m |  \u001b[32m0.9274\u001b[0m |  \u001b[32m0.9610\u001b[0m |   \u001b[31m0.9413\u001b[0m |      \u001b[31m0.9148\u001b[0m | 2.963e-03 |  14.91 |    52740 |  607.2181\n",
      "   11 |     \u001b[32m0.3014\u001b[0m |    \u001b[32m0.9650\u001b[0m |   \u001b[32m0.2214\u001b[0m |  \u001b[32m0.9286\u001b[0m |  \u001b[32m0.9620\u001b[0m |   \u001b[31m0.9405\u001b[0m |      \u001b[32m0.9178\u001b[0m | 2.370e-03 |  15.03 |    58014 |  609.1329\n",
      "   12 |     \u001b[32m0.2778\u001b[0m |    \u001b[32m0.9681\u001b[0m |   \u001b[32m0.2140\u001b[0m |  \u001b[32m0.9330\u001b[0m |  \u001b[32m0.9646\u001b[0m |   \u001b[32m0.9509\u001b[0m |      \u001b[31m0.9168\u001b[0m | 1.778e-03 |  15.11 |    63288 |  604.7876\n",
      "   13 |     \u001b[32m0.2585\u001b[0m |    \u001b[32m0.9704\u001b[0m |   \u001b[32m0.2059\u001b[0m |  \u001b[31m0.9307\u001b[0m |  \u001b[31m0.9634\u001b[0m |   \u001b[31m0.9431\u001b[0m |      \u001b[32m0.9195\u001b[0m | 1.185e-03 |  15.16 |    68562 |  605.7617\n",
      "   14 |     \u001b[32m0.2428\u001b[0m |    \u001b[32m0.9725\u001b[0m |   \u001b[32m0.2033\u001b[0m |  \u001b[31m0.9309\u001b[0m |  \u001b[31m0.9635\u001b[0m |   \u001b[31m0.9439\u001b[0m |      \u001b[31m0.9192\u001b[0m | 5.926e-04 |  15.18 |    73836 |  606.6561\n",
      "   15 |     \u001b[32m0.2297\u001b[0m |    \u001b[32m0.9741\u001b[0m |   \u001b[32m0.2024\u001b[0m |  \u001b[31m0.9307\u001b[0m |  \u001b[31m0.9635\u001b[0m |   \u001b[31m0.9442\u001b[0m |      \u001b[31m0.9185\u001b[0m | 0.000e+00 |  15.18 |    79110 |  606.6077\n",
      "Loading /home/ytaille/data/cache/norm/paper/train_step1/f5a5142e2d206a3c/checkpoint-15.pt... \n",
      "Model restored to its best state: 15\n",
      "Using cache /home/ytaille/data/cache/preprocess_training_data/0bc03df4bddfd317\n",
      "Loading /home/ytaille/data/cache/preprocess_training_data/0bc03df4bddfd317/output.pkl... \n",
      "Loading MRCONSO...\n",
      "Deduplicating MRCONSO...\n",
      "French synonyms: 174317\n",
      "French labels: 84172\n",
      "Quaero mentions: 5714\n",
      "Mirrored labels: 84845\n",
      "Queried english labels: 84845\n",
      "Adding all english concepts from SABs: ['CHV', 'SNOMEDCT_US', 'MTH', 'NCI', 'MSH']\n",
      "Total deduplicated synonyms: 2461183\n",
      "Total deduplicated labels: 766764\n",
      "Will train vocabulary for label\n",
      "Will train vocabulary for group\n",
      "Will train vocabulary for source\n",
      "Will train vocabulary for token\n",
      "Discovered existing vocabulary (105879 entities) for token\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Quaero mentions: 16283\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Normalized label, with given vocabulary and no unk\n",
      "Normalized quaero_source, with given vocabulary and no unk\n",
      "Normalized token, with given vocabulary and no unk\n",
      "Normalized group, with given vocabulary and no unk\n",
      "Normalized source, with given vocabulary and no unk\n",
      "Using cache /home/ytaille/data/cache/train_step2/4f04c72fd9228e6d\n",
      "Loading /home/ytaille/data/cache/train_step2/4f04c72fd9228e6d/output.pkl... \n",
      "epoch | val_acc | val_loss | acc_emea | acc_medline | train_loss | rescale |        lr\n",
      "    0 |  \u001b[32m0.7240\u001b[0m |     None |   \u001b[32m0.7226\u001b[0m |      \u001b[32m0.7251\u001b[0m |       None |    None |      None\n",
      "    1 |  \u001b[32m0.8022\u001b[0m |   \u001b[32m1.0305\u001b[0m |   \u001b[32m0.8304\u001b[0m |      \u001b[32m0.7806\u001b[0m |     \u001b[32m0.6735\u001b[0m | 20.0000 | 6.400e-03\n",
      "    2 |  \u001b[32m0.8036\u001b[0m |   \u001b[32m1.0140\u001b[0m |   \u001b[32m0.8330\u001b[0m |      \u001b[32m0.7809\u001b[0m |     \u001b[32m0.4534\u001b[0m | 20.0000 | 4.800e-03\n",
      "    3 |  \u001b[32m0.8051\u001b[0m |   \u001b[32m1.0132\u001b[0m |   \u001b[32m0.8352\u001b[0m |      \u001b[32m0.7819\u001b[0m |     \u001b[32m0.3939\u001b[0m | 20.0000 | 3.200e-03\n",
      "    4 |  \u001b[32m0.8064\u001b[0m |   \u001b[32m1.0076\u001b[0m |   \u001b[32m0.8365\u001b[0m |      \u001b[32m0.7833\u001b[0m |     \u001b[32m0.3633\u001b[0m | 20.0000 | 1.600e-03\n",
      "    5 |  \u001b[31m0.8062\u001b[0m |   \u001b[32m1.0076\u001b[0m |   \u001b[31m0.8356\u001b[0m |      \u001b[32m0.7836\u001b[0m |     \u001b[32m0.3439\u001b[0m | 20.0000 | 0.000e+00\n"
     ]
    }
   ],
   "source": [
    "from deep_multilingual_normalization.create_classifiers import create_classifiers\n",
    "\n",
    "classifier, classifier2 = create_classifiers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Dataset(\n",
      "  (docs): 1 * ('doc_id', 'text', 'split')\n",
      ")\n",
      "Transform texts... done\n",
      "Splitting into sentences... Tokenizing... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/nlstruct/text/chunking/regex.py:75: FutureWarning: split() requires a non-empty pattern match.\n",
      "  for i, part in enumerate(reg_split.split(txt)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "Computing vocabularies...\n",
      "Will train vocabulary for token\n",
      "Discovered existing vocabulary (105879 entities) for token\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Normalized split, with given vocabulary and no unk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/nlstruct/text/chunking/huggingface.py:11: FutureWarning: doc_id_col is not used anymore in the huggingface_tokenize function\n",
      "  warnings.warn(\"doc_id_col is not used anymore in the huggingface_tokenize function\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "bert_name = \"bert-base-multilingual-uncased\"\n",
    "\n",
    "dataset = load_from_brat(\"/home/ytaille/data/tmp/ws_inputs\")\n",
    "\n",
    "docs, sentences, tokens, deltas, vocs = preprocess(\n",
    "    dataset=dataset,\n",
    "    max_sentence_length=120,\n",
    "    bert_name=bert_name,\n",
    "    vocabularies=None,\n",
    ")\n",
    "\n",
    "prep = Dataset(\n",
    "    sentences=sentences,\n",
    "    tokens=tokens,\n",
    "    deltas=deltas,\n",
    ")\n",
    "\n",
    "batcher, encoded, ids = make_batcher(docs, sentences, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Dataset(\n",
      "  (docs): 1 * ('doc_id', 'text', 'split')\n",
      ")\n",
      "Transform texts... done\n",
      "Splitting into sentences... Tokenizing... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/nlstruct/text/chunking/regex.py:75: FutureWarning: split() requires a non-empty pattern match.\n",
      "  for i, part in enumerate(reg_split.split(txt)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "Computing vocabularies...\n",
      "Will train vocabulary for token\n",
      "Discovered existing vocabulary (105879 entities) for token\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Normalized split, with given vocabulary and no unk\n",
      "Normalized split, with given vocabulary and no unk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/nlstruct/text/chunking/huggingface.py:11: FutureWarning: doc_id_col is not used anymore in the huggingface_tokenize function\n",
      "  warnings.warn(\"doc_id_col is not used anymore in the huggingface_tokenize function\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "Available CUDA devices: 1\n",
      "Current device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 28.50it/s]\n"
     ]
    }
   ],
   "source": [
    "bert_name = \"bert-base-multilingual-uncased\"\n",
    "\n",
    "dataset = load_from_brat(\"/home/ytaille/data/tmp/ws_inputs\")\n",
    "\n",
    "docs, sentences, tokens, deltas, vocs = preprocess(\n",
    "    dataset=dataset,\n",
    "    max_sentence_length=120,\n",
    "    bert_name=bert_name,\n",
    "    vocabularies=None,\n",
    ")\n",
    "\n",
    "prep = Dataset(\n",
    "    sentences=sentences,\n",
    "    tokens=tokens,\n",
    "    deltas=deltas,\n",
    ")\n",
    "\n",
    "batcher, encoded, ids = make_batcher(docs, sentences, tokens)\n",
    "\n",
    "batch_size = 32\n",
    "with_tqdm = True\n",
    "with_groups = False\n",
    "topk = 1\n",
    "save_embeds = False\n",
    "\n",
    "tg.set_device('cuda:0')\n",
    "device = tg.device\n",
    "\n",
    "with evaluating(classifier):  # eval mode: no dropout, frozen batch norm, etc\n",
    "    with torch.no_grad():  # no gradients -> faster\n",
    "        with tqdm(batcher.dataloader(batch_size=batch_size, device=device, sparse_sort_on=\"token_mask\"), disable=not with_tqdm, mininterval=10.) as bar:\n",
    "            for batch in bar:\n",
    "                res = classifier2.forward(\n",
    "                    tokens=batch[\"token\"],\n",
    "                    mask=batch[\"token_mask\"],\n",
    "                    groups=batch[\"group\"] if with_groups else None,\n",
    "                    return_scores=topk > 0.1,\n",
    "                    return_embeds=save_embeds,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_probs = res['scores'].topk(dim=-1, k=topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[8.3149],\n",
       "        [9.0723],\n",
       "        [7.5563],\n",
       "        [8.5602]], device='cuda:0'),\n",
       "indices=tensor([[528324],\n",
       "        [488454],\n",
       "        [ 98788],\n",
       "        [192771]], device='cuda:0'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.3149],\n",
       "        [9.0723],\n",
       "        [7.5563],\n",
       "        [8.5602]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_probs.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_mult_norm",
   "language": "python",
   "name": "deep_mult_norm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
